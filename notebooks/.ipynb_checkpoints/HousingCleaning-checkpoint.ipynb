{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing DataSet Cleaning\n",
    "\n",
    "Benny Cohen\n",
    "\n",
    "10/26/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Housing Maintenance Code Violations found at https://data.cityofnewyork.us/Housing-Development/Housing-Maintenance-Code-Violations/wvxf-dwi5 describes several housing violations found in NY and provides details including where the incident was, (broken down by boro, and address), how severe it was (broken down into 3 classes, A,B,C where A is least severe and C the most severe)\n",
    "\n",
    "In this notebook we will clean the data:\n",
    "We need to...\n",
    "\n",
    "1. Filter for the rows we need\n",
    "2. deleting duplicate rows\n",
    "3. changing datatypes,\n",
    "4. Deal with missing data \n",
    "\n",
    "We then save the file to a csv for analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup, Thoughts about Our Data, and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://data.cityofnewyork.us/resource/wvxf-dwi5.csv?$limit=50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = df.shape\n",
    "print('There are {:d} rows and {:d} columns '.format(rows,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see several noteworthy columns that we want to keep. Let's filter for them now to make the rest of our analysis easier. \n",
    "Columns we want...\n",
    "1. violationid - unique identifier for incident. \n",
    "    A.In order to use this field we need to validate that...    \n",
    "        a.Each id truly is unique\n",
    "        b.There is only one id per incident\n",
    "        \n",
    "2. buildingid - unique identifier for building\n",
    "\n",
    "    A. We can see from this whether there are buildings with multiple incidents\n",
    "    \n",
    "    \n",
    "3. boro - This tells us which boro an incident took place in\n",
    "\n",
    "\n",
    "4. Zip - This tells us the zip code an incident took place in. \n",
    "\n",
    "    A. This field and burrow will help us find geographic trends in incidents\n",
    "    \n",
    "    B. We can use the zip field to verify that the boro field is correct\n",
    "    \n",
    "    \n",
    "5. InspectionDate - This tells us when an incident was opened.\n",
    "\n",
    "\n",
    "6. CertifiedDate - The date an incident was Corrected\n",
    "\n",
    "\n",
    "7. OriginalCorrectByDate - The day that an incident must be resolved by. \n",
    "\n",
    "\n",
    "8. NewCorrectByDate - If a postponment is granted this field will contain the date that the incident must be resolved by\n",
    "\n",
    "\n",
    "9. NOVDescription - Describes the violation\n",
    "\n",
    "\n",
    "10. Class - Tells how severe a violation is. A is most severe, B is medium, and C is low. \n",
    "\n",
    "\n",
    "11. CurrentStatus - The current status of the inspection (ie - OPEN, CLOSED...)\n",
    "\n",
    "\n",
    "12. CurrentStatusDate - The date when the status was updated to its current state\n",
    "\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\config\\columns.txt') as file:\n",
    "    columns = [line.rstrip().rstrip('\\n') for line in file]\n",
    "filteredData = df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's use the info method to see the columns count and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make notes about things we need to correct. \n",
    "\n",
    "1. Missing data: We note that we had 50000 rows and see that since count ignores nulls that most of the columns contain all the info. Some don't though. Zip code is missing one row. Also Certified date is missing a lot of rows. It might be of interest to see if this column overlaps with currentstatusdate and we therefore can get rid of it. Correct by date is missing many rows simply because there weren't that many postponments granted. We might want to filter these into their own file to look at. \n",
    "\n",
    "2. Datatypes: Date columns are going to need to be changed to datetime. We actually have no 'numeric' data because all of our numeric types are ids so there is no point in doing a df.describe to see numeric info.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.violationid.value_counts().sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the highest cound for an id is 1, confirming that ids are unique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The harder thing to check is do incidents get repeated under multiple ids.... First let's see if we have any duplicate incidents since that's easy... There are 2 ways to do this. We can use the drop duplicates function or we can merge the dataframes on the unique attributes. The first is most definetly faster but the second let's us explore the data more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.merge(filteredData,filteredData, on = ['violationid'], how = 'inner')\n",
    "joined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we join on the id column we get all the rows as we expected... since that is unique as we already showed Now let's join without the id column. We should get the same output if there is no duplicate data. If there is duplicate data we are going to get extra rows since each row would map to a row on something more than itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.merge(filteredData,filteredData, on = list(filteredData.columns)[1:], how = 'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameviolation = joined[joined['violationid_x'] != joined['violationid_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameviolation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 122 rows seem to contain duplicate data... Maybe let's look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameviolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One sample row\n",
    "df[df['violationid'] == 10005692]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['violationid'] == 10005758]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these really duplicates? Maybe something is unique about them? Maybe there is a field missing from the data? Maybe it's a collection problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(sameviolation['boro'].unique(), sameviolation['boro'].value_counts())\n",
    "plt.title(\"Incidents per boro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's unclear, but we can drop them since we have enough data anyways..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData = filteredData.set_index('violationid').sort_index() #might as well if we are treating these as ids...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData = filteredData.drop(sameviolation.violationid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay... Part 1 is done... now let's see if incidents are repeated... Let's show the alternate method...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.duplicated(subset = ['buildingid', 'boro', 'zip', 'inspectiondate', 'certifieddate',\n",
    "       'originalcorrectbydate', 'novdescription', 'class',\n",
    "       ]).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are duplicates... Now let's find them to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = filteredData[filteredData.duplicated(subset = ['buildingid', 'boro', 'zip', 'inspectiondate', 'certifieddate',\n",
    "       'originalcorrectbydate', 'novdescription', 'class',\n",
    "       ], keep = False)]\n",
    "dups.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't really what we were expecting to see. We see the same violation listed 2x with different statuses. It could also just be that these were seperate violations just given the same nov description. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in good 'shape' (pun intended) though because few such rows exist. \n",
    "\n",
    "There may in fact be multiple entries for the same incident but because we are taking a small subset of the data we don't have to deal with this problem as we have just shown. Let's just get rid of them since there are only a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData = filteredData.drop(dups.index, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correcting DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is convert the date columns to dates and zip to an int. The strings are encoded as categorical data properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData['currentstatusdate'] = pd.to_datetime(filteredData['currentstatusdate'])\n",
    "filteredData['inspectiondate'] = pd.to_datetime(filteredData['inspectiondate'])\n",
    "filteredData['originalcorrectbydate'] = pd.to_datetime(filteredData['originalcorrectbydate'])\n",
    "filteredData['newcorrectbydate'] = pd.to_datetime(filteredData['newcorrectbydate'])\n",
    "filteredData['currentstatusdate '] = pd.to_datetime(filteredData['currentstatusdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.inspectiondate.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they it is now a datetime (That is a datetime type). \n",
    "The last thing I want to adress are nulls. Let's see what columns have nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = filteredData.isnull().any()\n",
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData[filteredData['zip'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip looks like a data entry problem. We can leave it since the other data is usefull. The same is true of the originalcorrectbydate and newcorrectbydate fields. We simply can exclude them directly when we do the analysis. \n",
    "The newcorrectbydate is null when there are no postponments given to rectify the date. Those we can also leave since we are going to filter them to a file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reopened = filteredData[~filteredData.newcorrectbydate.isnull()]\n",
    "clean = filteredData[filteredData.newcorrectbydate.isnull()]\n",
    "\n",
    "clean.to_csv(\"../data/clean.csv\")\n",
    "reopened.to_csv(\"../data/reopened.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
